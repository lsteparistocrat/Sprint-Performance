      - name: Write extractor script (robust + no-expand)
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');
            fs.mkdirSync('scripts', { recursive: true });
            fs.writeFileSync('scripts/jira_remaining_points.py', String.raw`#!/usr/bin/env python3
import os, sys, csv, time, math, requests, datetime as dt
from urllib.parse import urljoin
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry

# -------- Config from env --------
JIRA_BASE_URL        = os.getenv("JIRA_BASE_URL")
JIRA_EMAIL           = os.getenv("JIRA_EMAIL")
JIRA_API_TOKEN       = os.getenv("JIRA_API_TOKEN")
SPRINT_ID            = os.getenv("SPRINT_ID")
SP_FIELD             = os.getenv("STORY_POINTS_FIELD")
TRACK_STATUS_NAMES   = [s.strip() for s in os.getenv("TRACK_STATUS_NAMES", "To Do,In Progress,Code Review").split(",")]
UAT_STATUS_NAME      = os.getenv("UAT_STATUS_NAME", "UAT")
TREAT_UAT_AS_DONE_PERMANENT = os.getenv("UAT_IS_PERMANENT", "true").lower() == "true"
OUT_DIR              = os.getenv("OUT_DIR", "data")
# Tuning knobs
HTTP_TIMEOUT         = int(os.getenv("HTTP_TIMEOUT", "120"))      # seconds
MAX_RETRIES          = int(os.getenv("MAX_RETRIES", "5"))
BACKOFF_FACTOR       = float(os.getenv("BACKOFF_FACTOR", "1.5"))
SLEEP_BETWEEN_ISSUES = float(os.getenv("SLEEP_BETWEEN_ISSUES", "0.2"))
LIMIT_ISSUES         = int(os.getenv("LIMIT_ISSUES", "0"))        # 0 = no limit

missing = [k for k,v in {
  "JIRA_BASE_URL":JIRA_BASE_URL, "JIRA_EMAIL":JIRA_EMAIL, "JIRA_API_TOKEN":JIRA_API_TOKEN,
  "STORY_POINTS_FIELD":SP_FIELD, "SPRINT_ID":SPRINT_ID
}.items() if not v]
if missing:
    print(f"Missing env vars: {', '.join(missing)}", file=sys.stderr); sys.exit(2)

# -------- HTTP session with robust retries --------
session = requests.Session()
session.auth = (JIRA_EMAIL, JIRA_API_TOKEN)
session.headers.update({"Accept":"application/json"})

retry = Retry(
    total=MAX_RETRIES,
    connect=MAX_RETRIES,
    read=MAX_RETRIES,
    backoff_factor=BACKOFF_FACTOR,
    status_forcelist=[429, 500, 502, 503, 504],
    allowed_methods=["GET", "HEAD", "OPTIONS"],
    respect_retry_after_header=True,
)
adapter = HTTPAdapter(max_retries=retry, pool_connections=50, pool_maxsize=50)
session.mount("https://", adapter)
session.mount("http://", adapter)

def get_json(url, params=None):
    # single place to call GET with longer timeout and clean errors
    r = session.get(url, params=params, timeout=HTTP_TIMEOUT)
    r.raise_for_status()
    return r.json()

def jira_url(path): return urljoin(JIRA_BASE_URL, path)

def get_sprint(sprint_id): 
    return get_json(jira_url(f"/rest/agile/1.0/sprint/{sprint_id}"))

def list_sprint_issues(sprint_id):
    issues, start_at = [], 0
    while True:
        data = get_json(jira_url(f"/rest/agile/1.0/sprint/{sprint_id}/issue"),
                        params={"maxResults":50, "startAt":start_at, "fields":"key"})
        issues.extend(data.get("issues", []))
        total = data.get("total", 0)
        maxr  = data.get("maxResults", 50)
        if start_at + maxr >= total: break
        start_at += maxr
    keys = [i["key"] for i in issues]
    return keys if LIMIT_ISSUES <= 0 else keys[:LIMIT_ISSUES]

def get_issue_fields(key):
    # fetch only the fields we need, no expand
    fields = ",".join(["status","created",SP_FIELD])
    data = get_json(jira_url(f"/rest/api/3/issue/{key}"), params={"fields":fields})
    return data

def get_issue_changelog_all(key):
    # fetch changelog via paginated endpoint (100 per page)
    histories = []
    start_at = 0
    while True:
        page = get_json(jira_url(f"/rest/api/3/issue/{key}/changelog"),
                        params={"startAt":start_at, "maxResults":100})
        vals = page.get("values", [])
        histories.extend(vals)
        total = page.get("total", len(histories))
        maxr  = page.get("maxResults", 100)
        if start_at + maxr >= total: break
        start_at += maxr
    return histories

# ---- Robust Jira timestamp -> date
def iso_to_date(iso_str: str) -> dt.date:
    s = iso_str.strip().replace("Z", "+00:00")
    if len(s) >= 5 and (s[-5] in ["+", "-"]) and s[-3] != ":":
        s = s[:-2] + ":" + s[-2:]
    d = dt.datetime.fromisoformat(s)
    return d.date()

def daterange(start, end):
    cur = start
    while cur <= end:
        yield cur
        cur += dt.timedelta(days=1)

def sprint_date_bounds(sprint):
    sd = sprint.get("startDate"); ed = sprint.get("endDate") or sprint.get("completeDate")
    if not sd or not ed:
        raise RuntimeError("Sprint missing startDate/endDate.")
    return iso_to_date(sd), iso_to_date(ed)

def extract_status_changes(histories):
    events=[]
    for h in histories:
        when = h.get("created")
        for item in h.get("items", []):
            if item.get("field") == "status":
                events.append((iso_to_date(when), item.get("fromString"), item.get("toString")))
    events.sort(key=lambda x: x[0])
    return events

def first_date_reaching_status(histories, target):
    for d, _f, to in extract_status_changes(histories):
        if to == target: return d
    return None

def status_on_date(initial_status, histories, day):
    current = initial_status
    for d, _f, to in extract_status_changes(histories):
        if d <= day: current = to
        else: break
    return current

def main():
    os.makedirs(OUT_DIR, exist_ok=True)
    sprint = get_sprint(SPRINT_ID)
    sprint_start, sprint_end = sprint_date_bounds(sprint)
    keys = list_sprint_issues(sprint["id"])
    print(f"Found {len(keys)} issues in sprint {sprint['name']} ({sprint['id']}).")
    days = list(daterange(sprint_start, sprint_end))

    rows=[]
    skipped=[]
    for idx, key in enumerate(keys, 1):
        try:
            issue = get_issue_fields(key)
            histories = get_issue_changelog_all(key)
        except requests.exceptions.ReadTimeout:
            print(f"WARNING: timeout on {key}; skipping.", file=sys.stderr)
            skipped.append(key)
            continue
        except requests.exceptions.RequestException as e:
            print(f"WARNING: request error on {key}: {e}; skipping.", file=sys.stderr)
            skipped.append(key)
            continue

        f = issue.get("fields", {})
        sp = f.get(SP_FIELD) or 0.0
        try: sp = float(sp)
        except: sp = 0.0
        initial_status = f["status"]["name"]
        created_date = iso_to_date(f["created"])
        hit_uat_on = first_date_reaching_status(histories, UAT_STATUS_NAME)

        for day in days:
            if day < created_date: continue
            if hit_uat_on and day >= hit_uat_on and TREAT_UAT_AS_DONE_PERMANENT:
                remain = 0.0
            else:
                status_today = status_on_date(initial_status, histories, day)
                if hit_uat_on and day >= hit_uat_on and not TREAT_UAT_AS_DONE_PERMANENT:
                    remain = 0.0 if status_today == UAT_STATUS_NAME else (sp if status_today in TRACK_STATUS_NAMES else 0.0)
                else:
                    remain = sp if status_today in TRACK_STATUS_NAMES else 0.0
            rows.append({
                "date": day.isoformat(),
                "issue": key,
                "status": status_on_date(initial_status, histories, day),
                "story_points": sp,
                "remaining_for_issue": remain
            })

        if SLEEP_BETWEEN_ISSUES > 0:
            time.sleep(SLEEP_BETWEEN_ISSUES)

    by_day={}
    for r in rows:
        by_day.setdefault(r["date"], 0.0)
        by_day[r["date"]] += r["remaining_for_issue"]

    out_path = os.path.join(OUT_DIR, f"sprint_{sprint['id']}_remaining.csv")
    with open(out_path, "w", newline="") as f:
        f.write("date,remaining_story_points\n")
        for d in sorted(by_day.keys()):
            f.write(f"{d},{by_day[d]:.2f}\n")

    details_path = os.path.join(OUT_DIR, f"sprint_{sprint['id']}_remaining_details.csv")
    with open(details_path, "w", newline="") as f:
        w = csv.DictWriter(f, fieldnames=["date","issue","status","story_points","remaining_for_issue"])
        w.writeheader()
        for r in sorted(rows, key=lambda x:(x["date"], x["issue"])):
            w.writerow(r)

    if skipped:
        print(f"Skipped {len(skipped)} issues due to timeouts/errors: {', '.join(skipped)}", file=sys.stderr)
    print(f"Wrote {out_path}")
    print(f"Wrote {details_path}")

if __name__ == "__main__":
    main()
`);
